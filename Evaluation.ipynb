{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Evaluation of model_1.4.4.2.ipynb","provenance":[],"collapsed_sections":["rkcLeFkUQOGp"]},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# to import .py module\n","import sys\n","import os\n","ROOT_DIR = os.path.abspath(os.curdir)[:-11]\n","sys.path.append(os.path.abspath(ROOT_DIR))"]},{"cell_type":"code","metadata":{"id":"LVv2QBRmP-RU"},"source":["import codecs\n","import json\n","import Preprocess\n","from Parameters import *\n","import numpy as np\n","import keras\n","import tensorflow as tf\n","import timeit\n","import keras.backend as K\n","import gc\n","import CRF\n","import unicodedata"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkcLeFkUQOGp"},"source":["#### Bert"]},{"cell_type":"code","metadata":{"id":"QrDu28vsQAu8"},"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import json\n","\n","def get_model(model_url, max_seq_length):\n","  labse_layer = hub.KerasLayer(model_url, trainable=True)\n","\n","  # Define input.\n","  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                         name=\"input_word_ids\")\n","  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                     name=\"input_mask\")\n","  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                      name=\"segment_ids\")\n","\n","  # LaBSE layer.\n","  pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n","\n","  # The embedding is l2 normalized.\n","  pooled_output = tf.keras.layers.Lambda(\n","      lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n","\n","  # Define model.\n","  return tf.keras.Model(\n","        inputs=[input_word_ids, input_mask, segment_ids],\n","        outputs=pooled_output), labse_layer\n","\n","max_seq_length = 64\n","labse_model, labse_layer = get_model(\n","    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Gx3MhTJQDZj"},"source":["import bert\n","\n","vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n","\n","def create_input(input_strings, tokenizer, max_seq_length):\n","\n","  input_ids_all, input_mask_all, segment_ids_all = [], [], []\n","  for input_string in input_strings:\n","    # Tokenize input.\n","    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n","    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n","    sequence_length = min(len(input_ids), max_seq_length)\n","\n","    # Padding or truncation.\n","    if len(input_ids) >= max_seq_length:\n","      input_ids = input_ids[:max_seq_length]\n","    else:\n","      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n","\n","    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n","\n","    input_ids_all.append(input_ids)\n","    input_mask_all.append(input_mask)\n","    segment_ids_all.append([0] * max_seq_length)\n","\n","  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)\n","\n","def encode(input_text):\n","  input_ids, input_mask, segment_ids = create_input(\n","    input_text, tokenizer, max_seq_length)\n","  return labse_model([input_ids, input_mask, segment_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqNelhISg0cp"},"source":["#### Load necessary file"]},{"cell_type":"code","metadata":{"id":"ZjNSeZ6ZQKfq"},"source":["import json\n","with open(TEST_DATA_FILE, encoding='utf8') as f:\n","  testset = json.load(f)\n","\n","norm_embeddings = np.load(NORM_EMBEDDING_FILE, allow_pickle=True)\n","NT_norm_embeddings = np.load(NT_NORM_EMBEDDING_FILE, allow_pickle=True)\n","\n","with open(file=NORM_ADDS_FILE, mode='r', encoding='utf-8') as f:\n","  NORM_ADDS = json.load(fp=f)\n","\n","with open(file=ID2id_FILE, mode='r', encoding='utf-8') as f:\n","  ID2id = json.load(fp=f)\n","\n","with open(file=id2ID_FILE, mode='r', encoding='utf-8') as f:\n","  id2ID = json.load(fp=f)\n","\n","with open(file=id2norm_add_FILE, mode='r', encoding='utf-8') as f:\n","  id2norm_add= json.load(fp=f)\n","  \n","dim = 772\n","num_of_norm = 34481\n","\n","# for a sample in trainset, get id of norm_add coresponding to noisy_add of this sample\n","def get_norm_id(sample):\n","  return list(sample['std_add'].keys())[0]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_85jg6gfIuw"},"source":["entities2index = {'street': 0, 'ward': 1, 'district': 2, 'city': 3}\n","\n","def create_type_add_vector(noisy_add):\n","  entities = CRF.detect_entity(noisy_add)\n","  type_add_vector = np.zeros((1,4))\n","  for entity in entities:\n","    if entity == 'name':\n","      pass\n","    else:\n","      index = entities2index[entity]\n","      type_add_vector[0, index] = 1\n","  return type_add_vector\n","\n","def concat(v,type_add_vector):\n","  return np.concatenate((v, type_add_vector), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQK9GUeAMo15"},"source":["#### Test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fni7qxnIVivq","executionInfo":{"status":"ok","timestamp":1609819824378,"user_tz":-420,"elapsed":10564174,"user":{"displayName":"Mai Lê Thị Huyền","photoUrl":"","userId":"09442805003645503903"}},"outputId":"9feb8f47-94c2-4e03-d004-3cfb42089765"},"source":["model = keras.models.load_model('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/Add_model_1.4.4.2/SNN_100_epoches.snn')\n","import timeit\n","start = timeit.default_timer()\n","count_ = 0\n","error_sample = dict()\n","error_sample['data'] = []\n","for sample in testset['data']:\n","  noisy_add = sample['noisy_add']\n","  noisy_add = unicodedata.normalize('NFC', noisy_add)\n","\n","  type_add_vector = create_type_add_vector(noisy_add)\n","  \n","  noisy_add = Preprocess.remove_punctuation(CRF.get_better_add(noisy_add)).lower()\n","  noisy_add_vector = concat(np.array(encode([noisy_add])), type_add_vector).reshape(dim,)\n","  noisy_add_vectors = np.full((num_of_norm, dim), noisy_add_vector)\n","  if noisy_add == reprocess.remove_tone_of_text(noisy_add):\n","    x = model.predict([noisy_add_vectors, NT_norm_embeddings]).reshape(num_of_norm,)\n","  else:\n","    x = model.predict([noisy_add_vectors, norm_embeddings]).reshape(num_of_norm,)\n","\n","  x = np.argmax(x, axis = 0)\n","  if str(ID2id[str(x)]) in sample['std_add']:\n","    count_ +=1\n","  else:\n","    error_sample['data'].append(sample)\n","  gc.collect()\n","\n","stop = timeit.default_timer()\n","\n","print('Time: ', stop - start) \n","print(count_)\n","# print(ID2id[str(x)])\n","with open('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/Add_model_1.4.4.2/error_sample_100_epoches.json', 'w', encoding='utf8') as f:\n","  json.dump(error_sample, f, ensure_ascii=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time:  12105.015693648\n","7614\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"USXVNmPnqGJy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611345116114,"user_tz":-420,"elapsed":12126116,"user":{"displayName":"Nam Cao Hải","photoUrl":"","userId":"05035974790318444424"}},"outputId":"196ad0f9-efde-40da-8e25-81d70c65dce1"},"source":["model = keras.models.load_model('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/Merge_model_1.4.4.2/SNN_100_epoches.snn')\n","import timeit\n","start = timeit.default_timer()\n","count_ = 0\n","error_sample = dict()\n","error_sample['data'] = []\n","for sample in testset['data']:\n","  noisy_add = sample['noisy_add']\n","  noisy_add = unicodedata.normalize('NFC', noisy_add)\n","\n","  type_add_vector = create_type_add_vector(noisy_add)\n","  \n","  noisy_add = preprocess.remove_punctuation(CRF.get_better_add(noisy_add)).lower()\n","  noisy_add_vector = concat(np.array(encode([noisy_add])), type_add_vector).reshape(dim,)\n","  noisy_add_vectors = np.full((num_of_norm, dim), noisy_add_vector)\n","  if noisy_add == preprocess.remove_tone_of_text(noisy_add):\n","    x = model.predict([noisy_add_vectors, NT_norm_embeddings]).reshape(num_of_norm,)\n","  else:\n","    x = model.predict([noisy_add_vectors, norm_embeddings]).reshape(num_of_norm,)\n","\n","  x = np.argmax(x, axis = 0)\n","  if str(ID2id[str(x)]) in sample['std_add']:\n","    count_ +=1\n","  else:\n","    error_sample['data'].append(sample)\n","  gc.collect()\n","\n","stop = timeit.default_timer()\n","\n","print('Time: ', stop - start) \n","print(count_)\n","# print(ID2id[str(x)])\n","with open('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/Merge_model_1.4.4.2/error_sample_100_epoches.json', 'w', encoding='utf8') as f:\n","  json.dump(error_sample, f, ensure_ascii=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time:  12051.532232747999\n","7608\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aw2ilpgjneMB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610718756942,"user_tz":-420,"elapsed":6298091,"user":{"displayName":"Nam Cao Hải","photoUrl":"","userId":"05035974790318444424"}},"outputId":"147f4fbb-6811-4dab-8d85-015cc1ae2e30"},"source":["model = keras.models.load_model('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/ElementWise_model_1.4.4.2/SNN_100_epoches.snn')\n","import timeit\n","start = timeit.default_timer()\n","count_ = 0\n","error_sample = dict()\n","error_sample['data'] = []\n","for sample in testset['data']:\n","  noisy_add = sample['noisy_add']\n","  noisy_add = unicodedata.normalize('NFC', noisy_add)\n","\n","  type_add_vector = create_type_add_vector(noisy_add)\n","  \n","  noisy_add = preprocess.remove_punctuation(CRF.get_better_add(noisy_add)).lower()\n","  noisy_add_vector = concat(np.array(encode([noisy_add])), type_add_vector).reshape(dim,)\n","  noisy_add_vectors = np.full((num_of_norm, dim), noisy_add_vector)\n","  if noisy_add == preprocess.remove_tone_of_text(noisy_add):\n","    x = model.predict([noisy_add_vectors, NT_norm_embeddings]).reshape(num_of_norm,)\n","  else:\n","    x = model.predict([noisy_add_vectors, norm_embeddings]).reshape(num_of_norm,)\n","\n","  x = np.argmax(x, axis = 0)\n","  if str(ID2id[str(x)]) in sample['std_add']:\n","    count_ +=1\n","  else:\n","    error_sample['data'].append(sample)\n","  gc.collect()\n","\n","stop = timeit.default_timer()\n","\n","print('Time: ', stop - start) \n","print(count_)\n","# print(ID2id[str(x)])\n","with open('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/ElementWise_model_1.4.4.2/error_sample_100_epoches.json', 'w', encoding='utf8') as f:\n","  json.dump(error_sample, f, ensure_ascii=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time:  11303.41619964\n","7751\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7EMPNLG7mPY","executionInfo":{"status":"ok","timestamp":1611357366580,"user_tz":-420,"elapsed":12250451,"user":{"displayName":"Nam Cao Hải","photoUrl":"","userId":"05035974790318444424"}},"outputId":"3119628b-e3bd-40f7-d947-c712b72c6010"},"source":["model = keras.models.load_model('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/AD_model_1.4.4.2/SNN_100_epoches.snn')\n","import timeit\n","start = timeit.default_timer()\n","count_ = 0\n","error_sample = dict()\n","error_sample['data'] = []\n","for sample in testset['data']:\n","  noisy_add = sample['noisy_add']\n","  noisy_add = unicodedata.normalize('NFC', noisy_add)\n","\n","  type_add_vector = create_type_add_vector(noisy_add)\n","  \n","  noisy_add = preprocess.remove_punctuation(CRF.get_better_add(noisy_add)).lower()\n","  noisy_add_vector = concat(np.array(encode([noisy_add])), type_add_vector).reshape(dim,)\n","  noisy_add_vectors = np.full((num_of_norm, dim), noisy_add_vector)\n","  if noisy_add == preprocess.remove_tone_of_text(noisy_add):\n","    x = model.predict([noisy_add_vectors, NT_norm_embeddings]).reshape(num_of_norm,)\n","  else:\n","    x = model.predict([noisy_add_vectors, norm_embeddings]).reshape(num_of_norm,)\n","\n","  x = np.argmax(x, axis = 0)\n","  if str(ID2id[str(x)]) in sample['std_add']:\n","    count_ +=1\n","  else:\n","    error_sample['data'].append(sample)\n","  gc.collect()\n","\n","stop = timeit.default_timer()\n","\n","print('Time: ', stop - start) \n","print(count_)\n","# print(ID2id[str(x)])\n","with open('/content/drive/My Drive/Norm_add_based_recommendation/Full/model_1.2/AD_model_1.4.4.2/error_sample_100_epoches.json', 'w', encoding='utf8') as f:\n","  json.dump(error_sample, f, ensure_ascii=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Time:  12244.963603577999\n","7855\n"],"name":"stdout"}]}]}